# -*- coding: utf-8 -*-
"""use_me_ocr.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/171OAOFL4r06F5iZFLi1XX8GkY9QGQc_2

# Installations
"""

# TensorFlow
# !pip install python-doctr[tf]
# PyTorch
!pip install python-doctr[torch]
# Restart runtime
exit()

# Colab related installations to install pyproject.toml projects correctly
!sudo apt install libcairo2-dev pkg-config
!pip3 install pycairo
# Install the most up-to-date version from GitHub
# TensorFlow
# !pip install python-doctr[tf]@git+https://github.com/mindee/doctr.git
# PyTorch
!pip3 install python-doctr[torch]@git+https://github.com/mindee/doctr.git
# Restart runtime
exit()

# Install some free fonts for result rendering
!sudo apt-get install fonts-freefont-ttf -y

"""# Libraries and Data"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import os

# Let's pick the desired backend
# os.environ['USE_TF'] = '1'
os.environ['USE_TORCH'] = '1'

import matplotlib.pyplot as plt

from doctr.io import DocumentFile
from doctr.models import ocr_predictor

import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

!unzip "/content/drive/MyDrive/OCR-SKU/Box Counting Details.zip"

"""# Code"""

predictor = ocr_predictor(pretrained=True)

my_path = '/content/Box Counting Details/Amul Powder/'
files = os.listdir(my_path)
print(files)

results = []
docs = []
cnt = 0

for f in files:
    for image in os.listdir(my_path+f):
        if image[-3:]=='jpg' or image[-3:]=='png':
            doc = DocumentFile.from_images(f"{my_path}{f}/{image}")
            result = predictor(doc)
            docs.append(doc)
            results.append(result)
            cnt+=1

        if cnt>=5:
            break
    if cnt>=5:
        break

print(len(results),len(docs))

for i in range(len(results)):
    results[i].show(docs[i])

result = results[0]
doc = docs[0]
# JSON export
json_export = result.export()
d = json_export
print(json_export)

len(d['pages'][0]['blocks'])

len(d['pages'][0]['blocks'][0]['lines'])

len(d['pages'][0]['blocks'][0]['lines'][0]['words'])

d['pages'][0]['blocks'][0]['lines'][0]['words'][0]['value']

for i in range(len(d['pages'][0]['blocks'])):
    print()
    for j in range(len(d['pages'][0]['blocks'][i]['lines'])):
        for k in range(len(d['pages'][0]['blocks'][i]['lines'][j]['words'])):
            print(d['pages'][0]['blocks'][i]['lines'][j]['words'][k]['value'], end = ' ')



doc = DocumentFile.from_images(f"/content/img3.jpg")
result = predictor(doc)

json_export = result.export()
d = json_export
print(json_export)

result.show(doc)

for i in range(len(d['pages'][0]['blocks'])):
    print()
    for j in range(len(d['pages'][0]['blocks'][i]['lines'])):
        for k in range(len(d['pages'][0]['blocks'][i]['lines'][j]['words'])):
            print(d['pages'][0]['blocks'][i]['lines'][j]['words'][k]['value'], end = ' ')

